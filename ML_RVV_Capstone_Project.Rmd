---
title: '**ML algorithm for Harvardx Capstone Course**'
subtitle: 'HarvardX Data Science Professional Certificate: PH125.9x Capstone 1'
author: "Rubén Vázquez del Valle"
date: "4/8/2021"
output: html_document
---


## Introduction

For this project, I will create a movie recommendation system using 10M version of the MovieLens dataset to make the computation a little easier.

To build my origin datasets I have downloaded the MovieLens data previously described and run the code provided by Harvardx Team.

Once dataset were created I have followed Bell and Koren's paper describing their final solution to the Netflix Prize included in the next link: http://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf.



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE)
```


Following libraries will be needed for the project:

```{r loading-libs, message=FALSE}

###############################################################################
##        Open required package libraries
###############################################################################

library(tidyverse)
library(ggplot2)
library(lubridate)
library(purrr)
library(stringr)
library(caret)
library(knitr)
library(scales)
library(RColorBrewer)
```

Following Functions will also be needed for the project

```{r loading-functions, message=FALSE}


###############################################################################
##       Functions specifically created for the project
###############################################################################


## RMSE function to compute Root Mean Squared errors between any pair of vectors 
RMSE <- function(true, predicted){
  sqrt(mean((true - predicted)^2))
}


## MAE function to compute Mean Absolute errors between any pair of vectors 
MAE <- function(true, predicted){
  mean(abs(true - predicted))
}


## substrRight function to substract n last characters on a string
substrRight <- function(x, n){
  substr(x, nchar(x)-n+1, nchar(x))
}


###############################################################################
##  Initialize parameters
###############################################################################
l1<-2.75
l2<-5
a<- -0.00075
b<- 0.4


## lambda1 function to regularize parameter λ1 according to The BellKor Solution pdf
lambda1<-function(l1){
  mu <- mean(train_set$rating)
  movie_avgs_reg <- train_set %>% 
    group_by(movieId) %>% 
    summarize(b_i = sum(rating - mu)/(n()+l1))
  
  predicted_ratings <- 
    test_set %>% 
    left_join(movie_avgs_reg, by='movieId') %>%
    mutate(pred = mu + b_i) %>%
    pull(pred)
  return(RMSE(predicted_ratings, test_set$rating))
}


## lambda2 function to regularize parameter λ2 according to The BellKor Solution pdf
lambda2<-function(l2){
  mu <- mean(train_set$rating)
  user_avgs <- train_set %>% 
    left_join(movie_avgs_reg, by='movieId') %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - mu - b_i_reg)/(n()+l2))
  
  predicted_ratings <- test_set %>% 
    left_join(movie_avgs_reg, by='movieId') %>%
    left_join(user_avgs, by='userId') %>%
    mutate(pred = mu + b_i_reg + b_u) %>%
    pull(pred)
  return(RMSE(predicted_ratings, test_set$rating))
}


## dev1 function to calculate α(u) on dev_u(t) (temporal effect on user bias) according to the BellKor Solution pdf after applying  bi(t) = bi +bi,Bin(t)
dev1<-function(a){
  movie_avgs_reg<- train_set %>% 
    group_by(movieId) %>% 
    summarize(b_i_reg = sum(rating - mu)/(n()+l1),first_rate_i=min(date))
  
  date_user_avgs_reg <- train_set %>% 
    left_join(movie_avgs_reg, by='movieId') %>%
    group_by(userId) %>%
    summarize(b_u_reg = sum(rating - mu - b_i_reg)/(n()+l2),t_u = mean(date))
  
  movie_avgs_reg_bin<-train_set %>% 
    left_join(movie_avgs_reg, by='movieId') %>%
    left_join(date_user_avgs_reg, by='userId')  %>%
    mutate(bin=case_when(difftime(date,first_rate_i,units="days")<=180 ~ 1,
               difftime(date,first_rate_i,units="days")>180 & difftime(date,first_rate_i,units="days")<=365 ~ 2,
               difftime(date,first_rate_i,units="days")>365 & difftime(date,first_rate_i,units="days")<=545 ~ 3,
               difftime(date,first_rate_i,units="days")>545 & difftime(date,first_rate_i,units="days")<=730 ~ 4,
               difftime(date,first_rate_i,units="days")>730 & difftime(date,first_rate_i,units="days")<=915 ~ 5,
               difftime(date,first_rate_i,units="days")>915 & difftime(date,first_rate_i,units="days")<=1095 ~ 6,
               difftime(date,first_rate_i,units="days")>1095 & difftime(date,first_rate_i,units="days")<=1280 ~ 7,
               difftime(date,first_rate_i,units="days")>1280 & difftime(date,first_rate_i,units="days")<=1465 ~ 8,
               difftime(date,first_rate_i,units="days")>1465 & difftime(date,first_rate_i,units="days")<=1645 ~ 9,
               difftime(date,first_rate_i,units="days")>1645~10)
    )%>%
    group_by(movieId,bin) %>% 
    summarize(b_i = mean(rating - mu - b_i_reg - b_u_reg))
  
  
  predicted_ratings <- test_set %>% 
    left_join(movie_avgs_reg, by='movieId') %>%
    mutate(bin=case_when(difftime(date,first_rate_i,units="days")<=180 ~ 1,
               difftime(date,first_rate_i,units="days")>180 & difftime(date,first_rate_i,units="days")<=365 ~ 2,
               difftime(date,first_rate_i,units="days")>365 & difftime(date,first_rate_i,units="days")<=545 ~ 3,
               difftime(date,first_rate_i,units="days")>545 & difftime(date,first_rate_i,units="days")<=730 ~ 4,
               difftime(date,first_rate_i,units="days")>730 & difftime(date,first_rate_i,units="days")<=915 ~ 5,
               difftime(date,first_rate_i,units="days")>915 & difftime(date,first_rate_i,units="days")<=1095 ~ 6,
               difftime(date,first_rate_i,units="days")>1095 & difftime(date,first_rate_i,units="days")<=1280 ~ 7,
               difftime(date,first_rate_i,units="days")>1280 & difftime(date,first_rate_i,units="days")<=1465 ~ 8,
               difftime(date,first_rate_i,units="days")>1465 & difftime(date,first_rate_i,units="days")<=1645 ~ 9,
               difftime(date,first_rate_i,units="days")>1645~10)
    )  %>%
    left_join(date_user_avgs_reg, by='userId')%>%
    left_join(movie_avgs_reg_bin, by=c('movieId','bin'))%>%
    mutate(dev_u_t=as.numeric(difftime(date,t_u,units="days"))) %>%
    mutate(dev_u_t=a*sign(dev_u_t)*(abs(dev_u_t)^b)) %>%
    mutate(pred = mu + b_i_reg + b_u_reg + ifelse(is.na(b_i),0,b_i)+dev_u_t) %>%         
    pull(pred)
  return(RMSE(predicted_ratings, test_set$rating))
}


## dev2 function to calculate β(u) on dev_u(t) (temporal effect on user bias) according to the BellKor Solution pdf after applying movies bias reg as time function bi(t) = bi +bi,Bin(t)
dev2<-function(b){
  movie_avgs_reg<- train_set %>% 
    group_by(movieId) %>% 
    summarize(b_i_reg = sum(rating - mu)/(n()+l1),first_rate_i=min(date))
  
  date_user_avgs_reg <- train_set %>% 
    left_join(movie_avgs_reg, by='movieId') %>%
    group_by(userId) %>%
    summarize(b_u_reg = sum(rating - mu - b_i_reg)/(n()+l2),t_u = mean(date))
  
  movie_avgs_reg_bin<-train_set %>% 
    left_join(movie_avgs_reg, by='movieId') %>%
    left_join(user_avgs_reg, by='userId')  %>%
    mutate(bin=case_when(difftime(date,first_rate_i,units="days")<=180 ~ 1,
               difftime(date,first_rate_i,units="days")>180 & difftime(date,first_rate_i,units="days")<=365 ~ 2,
               difftime(date,first_rate_i,units="days")>365 & difftime(date,first_rate_i,units="days")<=545 ~ 3,
               difftime(date,first_rate_i,units="days")>545 & difftime(date,first_rate_i,units="days")<=730 ~ 4,
               difftime(date,first_rate_i,units="days")>730 & difftime(date,first_rate_i,units="days")<=915 ~ 5,
               difftime(date,first_rate_i,units="days")>915 & difftime(date,first_rate_i,units="days")<=1095 ~ 6,
               difftime(date,first_rate_i,units="days")>1095 & difftime(date,first_rate_i,units="days")<=1280 ~ 7,
               difftime(date,first_rate_i,units="days")>1280 & difftime(date,first_rate_i,units="days")<=1465 ~ 8,
               difftime(date,first_rate_i,units="days")>1465 & difftime(date,first_rate_i,units="days")<=1645 ~ 9,
               difftime(date,first_rate_i,units="days")>1645~10)
    )%>%
    group_by(movieId,bin) %>% 
    summarize(b_i = mean(rating - mu - b_i_reg - b_u_reg))
  
  
  predicted_ratings <- test_set %>% 
    left_join(movie_avgs_reg, by='movieId') %>%
    mutate(bin=case_when(difftime(date,first_rate_i,units="days")<=180 ~ 1,
               difftime(date,first_rate_i,units="days")>180 & difftime(date,first_rate_i,units="days")<=365 ~ 2,
               difftime(date,first_rate_i,units="days")>365 & difftime(date,first_rate_i,units="days")<=545 ~ 3,
               difftime(date,first_rate_i,units="days")>545 & difftime(date,first_rate_i,units="days")<=730 ~ 4,
               difftime(date,first_rate_i,units="days")>730 & difftime(date,first_rate_i,units="days")<=915 ~ 5,
               difftime(date,first_rate_i,units="days")>915 & difftime(date,first_rate_i,units="days")<=1095 ~ 6,
               difftime(date,first_rate_i,units="days")>1095 & difftime(date,first_rate_i,units="days")<=1280 ~ 7,
               difftime(date,first_rate_i,units="days")>1280 & difftime(date,first_rate_i,units="days")<=1465 ~ 8,
               difftime(date,first_rate_i,units="days")>1465 & difftime(date,first_rate_i,units="days")<=1645 ~ 9,
               difftime(date,first_rate_i,units="days")>1645~10)
    )  %>%
    left_join(date_user_avgs_reg, by='userId')%>%
    left_join(movie_avgs_reg_bin, by=c('movieId','bin'))%>%
    mutate(dev_u_t=as.numeric(difftime(date,t_u,units="days"))) %>%
    mutate(dev_u_t=a*sign(dev_u_t)*(abs(dev_u_t)^b)) %>%
    mutate(pred = mu + b_i_reg + b_u_reg + ifelse(is.na(b_i),0,b_i)+dev_u_t) %>%         
    pull(pred)
  return(RMSE(predicted_ratings, test_set$rating))
}

```

\newpage

 Harvardx Team Code provided to build original dataset is the following one: 

```{r loading-code, message=FALSE}

##########################################################
# Create edx set, validation set (final hold-out test set)
##########################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)



```


\newpage  

## Exploratory Analysis

Let's start by exploring and summarizing the edx dataset:


```{r Exploratory Analysis, message=FALSE}

head(edx)
summary(edx)

```


It can be observed that edx dataset is a data.frame containing `r format(nrow(edx),big.mark=",",scientific=F)` rows and `r format(ncol(edx),big.mark=",",scientific=F)` columns, with ratings provided by a total of `r format(n_distinct(edx$userId), big.mark=",",scientific=F)` different users (that I will refer as "u"s) for a total of `r format(n_distinct(edx$movieId),big.mark=",",scientific=F)` different movies (that I will refer as "i"s). Generally speaking cartesian product of these two sets would consist on about `r round((n_distinct(edx$userId)*n_distinct(edx$movieId))/1e+06)` million ratings. Hence, it is definetively proved that not every user has rated every movie.

Clearly,there are more pieces of information than the movie, the user, and the ratings. There are three columns more including the timestamp (date of the rating done by user u on movie i), the title (a character vector including the release year at the end of the string) and finally a segmentation of the movie by genre (actually, each movie can be included in more than one cathegory)

Taking a look at previous information and at the distribution of the ratings

```{r Exploratory Analysis 2, message=FALSE}

edx %>%
  group_by(rating) %>%
  summarize(count = n()) %>%
  ggplot(aes(x = rating, y = count)) +
  geom_line() +
  ggtitle("Number of occurence of each rating") +
  labs(caption = "Figure 1")

```


\n it can be appreciated several things:

    1. The rating given more often is 4*
    2. There is no 0* rating in this dataset
    3. There seems to be a pattern to prefer integer ratings over rational ratings


Taking a look at the distribution of the number of ratings per movie

```{r Exploratory Analysis 3, message=FALSE}

edx %>%
  group_by(movieId) %>%
  summarize(n = n())%>%
  ggplot(aes(x = n)) +
  geom_histogram(bins=15)  + scale_fill_brewer(palette="Blues") +  scale_x_log10()+
  ggtitle("Distribution of number of ratings per movie") +
  labs(caption = "Figure 2")


```

and at the distribution of the number of ratings per user


```{r Exploratory Analysis 4, message=FALSE}

edx %>%
  group_by(userId) %>%
  summarize(n = n())%>%
  ggplot(aes(x = n)) +
  geom_histogram(bins=15)  + scale_fill_brewer(palette="Blues") + scale_x_log10()+
  ggtitle("Distribution of number of ratings per user") +
  labs(caption = "Figure 3")


```

\n it is obvious that:



    5. Some movies get rated more than others, and
    6. Some users are more active than others
    

Hence, from this introductory analysis, some transformations seem to be needed on edx dataset


\newpage  

## Transforming and partitioning edx dataset

Timestamp is not in a human readable format and release year is included in title column, so let's transform timestamp into a more suitable format and store it into a column called date and extract release year from title to a new column called year

```{r Transforming edx dataset, message=FALSE}


edx<-edx %>% mutate( date = as_datetime(timestamp),year=as.numeric(str_extract(str_extract(substrRight(title,6),"\\([^()]+.\\d"),"\\d+\\d")))
head(edx)

```


Now, I am ready to create a train and a test set to assess the accuracy of the models I will implement. 

```{r Partitioning edx dataset, message=FALSE}

## Selecting a random seed to allow replicability
set.seed(1978, sample.kind="Rounding")


## Creating training and testing partitions on edx movielens dataset 

test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.2, 
                                  list = FALSE)
train_set <- edx[-test_index,]
test_set <- edx[test_index,]


## Making testing partition comparable by taking out movies and users not present on training partition   

test_set <-test_set %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")


```

And finally I can start to implement the candidate models

\newpage  

## Modelling

Bell and Koren's paper describing their final solution reflects on different machine learning techniques:

    I   BASELINE PREDICTORS
    II  MATRIX FACTORIZATION WITH TEMPORAL DYNAMICS
    III NEIGHBORHOOD MODELS WITH TEMPORAL DYNAMICS
    IV  EXTENSIONS TO RESTRICTED BOLTZMANN MACHINES
    V   GRADIENT BOOSTING DECISSION TRESS BLENDING
    
concluding


    The science of recommender systems is a prime beneficiary of the contest. Many new people became involved in the field and made their contributions. There is a clear spike in related publications, and the Netflix dataset is the direct catalyst to developing some of the better algorithms known in the field. Out of the numerous new algorithmic contributions, I would like to highlight one – those humble baseline predictors (or biases), which capture main effects in the data. While the literature mostly concentrates on the more sophisticated algorithmic aspects, we have learned that an accurate treatment of main effects is probably at least as significant as coming up with modeling breakthroughs.
    
    
Through this project I will try to follow some of this techniques and the explanations given in *"HarvardX - PH125.8x course: Data Science - Machine Learning"* specially on recommendation systems chapter, to obtain a model as good as possible, according to my skills 


In doing so, the first baseline stage will be the naive one, just only calculating the average stars

$$Y_{u,i}=\mu+\epsilon_{u,i}$$  

```{r Baseline 1 Naive, message=FALSE}
###############################################################################
##        Modeling
##        I.) BASELINE
###############################################################################

################## 1. Adding Naive model
mu_hat <- mean(train_set$rating)
naive_rmse <- RMSE(test_set$rating, mu_hat)
naive_mae <- MAE(test_set$rating, mu_hat)

## Save results to table
Results <- tibble(method = "Naive", RMSE = naive_rmse, MAE = naive_mae)
Results

```

As expected results are still far from being acceptable so next step will be including bias introduced by movies:

$$Y_{u,i}=\mu+b_{i}+\epsilon_{u,i}$$  

```{r Baseline 2 movies bias, message=FALSE}
################## 2. Adding movies bias

mu <- mean(train_set$rating) 
movie_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))

qplot(b_i, data = movie_avgs, bins = 10, color = I("black")) +
  labs(caption = "Figure 4")


predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  mutate(pred = mu + b_i) %>%
  pull(pred)
movbias_rmse<-RMSE(predicted_ratings, test_set$rating)
movbias_mae<-MAE(predicted_ratings, test_set$rating)

## Save results to table
Results<-rbind(Results,tibble(method = "Movie Bias", RMSE = movbias_rmse, MAE = movbias_mae))
Results

```

Results seem better now but still far from acceptable

Following recommendations made by The BellKor Solution pdf, averages are shrunk towards zero by using the regularization parameters, $λ_1,λ_2$, which are determined by validation on the test set. For this reason I created the functions lambda1 and lambda2 which will allow me to identify the best values of $λ_1,λ_2$. Note that this code has been run several times prior to its final release so in terms to avoid issues with the final deploy I have decided to create some initial parameters (with already calculated values) to prevent any unexpected problem.  


```{r Baseline 2.1 movies bias + regularization, message=FALSE}
################## 2.1. Adding movies bias regularized 

l <- seq(0, 20, 0.25)
l1<-l[which.min(sapply(l,lambda1))]
l1

movie_avgs_reg<- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i_reg = sum(rating - mu)/(n()+l1),first_rate_i=min(date))

predicted_ratings <- 
  test_set %>% 
  left_join(movie_avgs_reg, by='movieId') %>%
  mutate(pred = mu + b_i_reg) %>%
  pull(pred)

movbiasreg_rmse<-RMSE(predicted_ratings, test_set$rating)
movbiasreg_mae<-MAE(predicted_ratings, test_set$rating)


## Save results to table
Results<-rbind(Results,tibble(method = "Movie Bias regularized", RMSE = movbiasreg_rmse, MAE = movbiasreg_mae))
Results
```

Regularization does not seem to improve too much, but at least improves so I decided to keep it as part of my model.
Next step will be to calculate user bias:

$$Y_{u,i}=\mu+b_{i}+b_{u}+\epsilon_{u,i}$$  


```{r Baseline 3 movies bias reg + user bias, message=FALSE}
################## 3.Adding user bias

user_avgs <- train_set %>% 
  left_join(movie_avgs_reg, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i_reg))

qplot(b_u, data = user_avgs, bins = 10, color = I("black")) +
  labs(caption = "Figure 5")

predicted_ratings <- test_set %>% 
  left_join(movie_avgs_reg, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i_reg + b_u) %>%
  pull(pred)
movuserbias_rmse<-RMSE(predicted_ratings, test_set$rating)
movuserbias_mae<-MAE(predicted_ratings, test_set$rating)

## Save results to table
Results<-rbind(Results,tibble(method = "Movie Bias reg + User bias", RMSE = movuserbias_rmse,MAE = movuserbias_mae))
Results

```


Results are still improving, but not enough. Let`s try also regularization on user bias 

```{r Baseline 3.1 movies bias reg + user bias reg, message=FALSE}
################## 3.1. Adding user bias regularized 

l <- seq(5, 25, 1)
l2<-l[which.min(sapply(l,lambda2))]
l2

user_avgs_reg <- train_set %>% 
  left_join(movie_avgs_reg, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u_reg = sum(rating - mu - b_i_reg)/(n()+l2),t_u = mean(date))

predicted_ratings <- test_set %>% 
  left_join(movie_avgs_reg, by='movieId') %>%
  left_join(user_avgs_reg, by='userId') %>%
  mutate(pred = mu + b_i_reg + b_u_reg) %>%
  pull(pred)
movuserbiasreg_rmse<-RMSE(predicted_ratings, test_set$rating)
movuserbiasreg_mae<-MAE(predicted_ratings, test_set$rating)

## Save results to table
Results<-rbind(Results,tibble(method = "Movie Bias reg + User bias reg", RMSE = movuserbiasreg_rmse,MAE = movuserbiasreg_mae))
Results

```

The same as movie bias regularization, user bias regularization does not seem to improve too much, but at least improves a little bit so I also decided to keep it as part of my model.

Following recommendations made by The “BellKor’s Pragmatic Chaos” final solution, the users tend to change their baseline ratings over time including a natural drift in a user’s rating scale, and moreover, item’s popularity may also change over time.

These effects can be observed on the next plots

```{r Baseline reflexion on time effect, message=FALSE}

edx %>% mutate(date = round_date(date, unit = "week")) %>%
  group_by(date) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(date, rating)) +
  geom_point() +
  geom_smooth() +
  labs(caption = "Figure 6")

edx %>% mutate(date = round_date(date, unit = "month")) %>%
  group_by(date) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(date, rating)) +
  geom_point() +
  geom_smooth() +
  labs(caption = "Figure 7")

```


so following their path I have built first of all time bias $b_{i}$ and secondly user bias $b_{u}$ as functions of time.

They decided to split the item biases into time-based bins, using a constant item bias for each time period:
$$b_{i}(t) = bi +b_{i,Bin(t)}$$

In their paper they state that to choose the bins they have considered periods of time of around thirty weeks, so based on this approach and in ordet to simplify things I split bins in around half a year each. To be completely right those decisions should also be optimized, but due to the computational time taken and small profit obtained I decided to keep this arbitrary ones


and to calcuate user biases as a function of time they used the next function:
$$dev_{u}(t)=α_{u}·sign(t −t_{u})·|t −t_{u}|^\beta$$
where α and β are determined by validation on the test set. In this case I took the same β referred in the paper and then tried to optimize them via dev1 and dev2 functions

And this is exactly why I also tried.


```{r Baseline  4.1  movie rating date bias , message=FALSE}
################## 4.1 Adding movies bias regularized as a function of time segmented in bins
## Following recommendations made by The BellKor Solution pdf bi(t) = bi +bi,Bin(t) 
movie_avgs_reg_bin<-train_set %>% 
  left_join(movie_avgs_reg, by='movieId') %>%
  left_join(user_avgs_reg, by='userId')  %>%
  mutate(bin=case_when(difftime(date,first_rate_i,units="days")<=180 ~ 1,
                  difftime(date,first_rate_i,units="days")>180 & difftime(date,first_rate_i,units="days")<=365 ~ 2,
                  difftime(date,first_rate_i,units="days")>365 & difftime(date,first_rate_i,units="days")<=545 ~ 3,
                  difftime(date,first_rate_i,units="days")>545 & difftime(date,first_rate_i,units="days")<=730 ~ 4,
                  difftime(date,first_rate_i,units="days")>730 & difftime(date,first_rate_i,units="days")<=915 ~ 5,
                  difftime(date,first_rate_i,units="days")>915 & difftime(date,first_rate_i,units="days")<=1095 ~ 6,
                  difftime(date,first_rate_i,units="days")>1095 & difftime(date,first_rate_i,units="days")<=1280 ~ 7,
                  difftime(date,first_rate_i,units="days")>1280 & difftime(date,first_rate_i,units="days")<=1465 ~ 8,
                  difftime(date,first_rate_i,units="days")>1465 & difftime(date,first_rate_i,units="days")<=1645 ~ 9,
                  difftime(date,first_rate_i,units="days")>1645~10)
  )%>%
  group_by(movieId,bin) %>% 
  summarize(b_i = mean(rating - mu - b_i_reg - b_u_reg))

qplot(b_i, data = movie_avgs_reg_bin, bins = 10, color = I("black"))+
  labs(caption = "Figure 8")

predicted_ratings <- test_set %>% 
  left_join(movie_avgs_reg, by='movieId') %>%
  mutate(bin=case_when(difftime(date,first_rate_i,units="days")<=180 ~ 1,
                  difftime(date,first_rate_i,units="days")>180 & difftime(date,first_rate_i,units="days")<=365 ~ 2,
                  difftime(date,first_rate_i,units="days")>365 & difftime(date,first_rate_i,units="days")<=545 ~ 3,
                  difftime(date,first_rate_i,units="days")>545 & difftime(date,first_rate_i,units="days")<=730 ~ 4,
                  difftime(date,first_rate_i,units="days")>730 & difftime(date,first_rate_i,units="days")<=915 ~ 5,
                  difftime(date,first_rate_i,units="days")>915 & difftime(date,first_rate_i,units="days")<=1095 ~ 6,
                  difftime(date,first_rate_i,units="days")>1095 & difftime(date,first_rate_i,units="days")<=1280 ~ 7,
                  difftime(date,first_rate_i,units="days")>1280 & difftime(date,first_rate_i,units="days")<=1465 ~ 8,
                  difftime(date,first_rate_i,units="days")>1465 & difftime(date,first_rate_i,units="days")<=1645 ~ 9,
                  difftime(date,first_rate_i,units="days")>1645~10)
  )  %>%
  left_join(user_avgs_reg, by='userId')%>%
  left_join(movie_avgs_reg_bin, by=c('movieId','bin'))%>%
  mutate(pred = mu + b_i_reg + b_u_reg + ifelse(is.na(b_i),0,b_i)) %>%         
  pull(pred)
movbiasregbin_rmse<-RMSE(predicted_ratings, test_set$rating) 
movbiasregbin_mae<-MAE(predicted_ratings, test_set$rating)



## Save results to table
Results<-rbind(Results,tibble(method = "Movie Bias reg + User bias reg+ time effect on movie Bias reg", RMSE = movbiasregbin_rmse, MAE = movbiasregbin_mae))
Results
```

Indeed this time effect is allowing to improve a little bit also


```{r Baseline  4.2  user rating date bias , message=FALSE}
## Save results to table
Results<-rbind(Results,tibble(method = "Movie Bias reg + User bias reg+ time effect on movie Bias reg", RMSE = movbiasregbin_rmse, MAE = movbiasregbin_mae))


################## 4.2 Adding user rating date bias  

##
## Following recommendations made by The BellKor Solution pdf, the users tend to change their baseline ratings over time including
## a natural drift in a user’s rating scale, so we build user bias bu as a function of time dev_u(t) = α·sign(t −tu)·|t −tu|^β
## α,β are determined by validation on the test set

a<- seq(-0.0015,0.0005,0.00025)
b<-0.4                               # initial β proposed in The BellKor Solution paper
devut<-sapply(a,dev1)
a<-a[which.min(devut)]
a

b <- seq(0, 1,0.1)
devut<-sapply(b,dev2)
b<-b[which.min(devut)]
b


## We took advantage of user_avgs_reg not only for calculating b_u_reg but also t_u, so we do not need to calculate it here
predicted_ratings <- test_set %>% 
  left_join(movie_avgs_reg, by='movieId') %>%
  mutate(bin=case_when(difftime(date,first_rate_i,units="days")<=180 ~ 1,
                  difftime(date,first_rate_i,units="days")>180 & difftime(date,first_rate_i,units="days")<=365 ~ 2,
                  difftime(date,first_rate_i,units="days")>365 & difftime(date,first_rate_i,units="days")<=545 ~ 3,
                  difftime(date,first_rate_i,units="days")>545 & difftime(date,first_rate_i,units="days")<=730 ~ 4,
                  difftime(date,first_rate_i,units="days")>730 & difftime(date,first_rate_i,units="days")<=915 ~ 5,
                  difftime(date,first_rate_i,units="days")>915 & difftime(date,first_rate_i,units="days")<=1095 ~ 6,
                  difftime(date,first_rate_i,units="days")>1095 & difftime(date,first_rate_i,units="days")<=1280 ~ 7,
                  difftime(date,first_rate_i,units="days")>1280 & difftime(date,first_rate_i,units="days")<=1465 ~ 8,
                  difftime(date,first_rate_i,units="days")>1465 & difftime(date,first_rate_i,units="days")<=1645 ~ 9,
                  difftime(date,first_rate_i,units="days")>1645~10)
  )  %>%
  left_join(user_avgs_reg, by='userId')%>%
  left_join(movie_avgs_reg_bin, by=c('movieId','bin'))%>%
  mutate(dev_u_t=as.numeric(difftime(date,t_u,units="days"))) %>%
  mutate(dev_u_t=a*sign(dev_u_t)*(abs(dev_u_t)^b)) %>%
  mutate(pred = mu + b_i_reg + b_u_reg + ifelse(is.na(b_i),0,b_i)+dev_u_t) %>%         
  pull(pred)
usertimebias_rmse<-RMSE(predicted_ratings, test_set$rating)
usertimebias_mae<-MAE(predicted_ratings, test_set$rating)

## Save results to table
Results<-rbind(Results,tibble(method = "Movie Bias reg + User bias reg+ time effect on movie Bias reg + dev_u(t)", RMSE = usertimebias_rmse,MAE = usertimebias_mae))
Results

```

Comparing Figure 4, Figure 5 and Figure 8 and also RMSE and MAE calculated for each of those process it is possible to have a slight idea of the influence of each of those bias on final predictions. .  
