---
title: '**ML algorithm for Harvardx Capstone Course**'
subtitle: 'HarvardX Data Science Professional Certificate: PH125.9x Capstone 1'
author: "Rubén Vázquez del Valle"
date: "4/8/2021"
output: html_document
---


## Introduction

For this project, I will create a movie recommendation system using 10M version of the MovieLens dataset to make the computation a little easier.

To build my origin datasets I have downloaded the MovieLens data previously described and run the code provided by Harvardx Team.

Once dataset were created I have followed Bell and Koren's paper describing their final solution to the Netflix Prize included in the next link: http://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf.



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Following libraries will be needed for the project:

```{r loading-libs, message=FALSE}

###############################################################################
##        Open required package libraries
###############################################################################

library(tidyverse)
library(ggplot2)
library(lubridate)
library(purrr)
library(stringr)
library(caret)
library(knitr)
library(scales)
```

Following Functions will also be needed for the project

```{r loading-functions, message=FALSE}


###############################################################################
##       Functions specifically created for the project
###############################################################################


## RMSE function to compute Root Mean Squared errors between any pair of vectors 
RMSE <- function(true, predicted){
  sqrt(mean((true - predicted)^2))
}


## MAE function to compute Mean Absolute errors between any pair of vectors 
MAE <- function(true, predicted){
  mean(abs(true - predicted))
}


## substrRight function to substract n last characters on a string
substrRight <- function(x, n){
  substr(x, nchar(x)-n+1, nchar(x))
}


###############################################################################
##  Initialize parameters
###############################################################################
l1<-2.75
l2<-5
a<- -0.00075
b<- 0.4


## lambda1 function to regularize parameter λ1 according to The BellKor Solution pdf
lambda1<-function(l1){
  mu <- mean(train_set$rating)
  movie_avgs_reg <- train_set %>% 
    group_by(movieId) %>% 
    summarize(b_i = sum(rating - mu)/(n()+l1))
  
  predicted_ratings <- 
    test_set %>% 
    left_join(movie_avgs_reg, by='movieId') %>%
    mutate(pred = mu + b_i) %>%
    pull(pred)
  return(RMSE(predicted_ratings, test_set$rating))
}


## lambda2 function to regularize parameter λ2 according to The BellKor Solution pdf
lambda2<-function(l2){
  mu <- mean(train_set$rating)
  user_avgs <- train_set %>% 
    left_join(movie_avgs_reg, by='movieId') %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - mu - b_i_reg)/(n()+l2))
  
  predicted_ratings <- test_set %>% 
    left_join(movie_avgs_reg, by='movieId') %>%
    left_join(user_avgs, by='userId') %>%
    mutate(pred = mu + b_i_reg + b_u) %>%
    pull(pred)
  return(RMSE(predicted_ratings, test_set$rating))
}


## dev1 function to calculate α(u) on dev_u(t) (temporal effect on user bias) according to the BellKor Solution pdf 
dev1<-function(a){
  movie_avgs_reg<- train_set %>% 
    group_by(movieId) %>% 
    summarize(b_i_reg = sum(rating - mu)/(n()+l1),first_rate_i=min(date))
  
  date_user_avgs_reg <- train_set %>% 
    left_join(movie_avgs_reg, by='movieId') %>%
    group_by(userId) %>%
    summarize(b_u_reg = sum(rating - mu - b_i_reg)/(n()+l2),t_u = mean(date))
  
  predicted_ratings <- test_set%>% mutate(date = round_date(date, unit = "day")) %>%
    left_join(movie_avgs_reg, by='movieId') %>%
    left_join(date_user_avgs_reg, by='userId') %>%
    mutate(dev_u_t=as.numeric(difftime(date,t_u,units="days"))) %>%
    mutate(dev_u_t=a*sign(dev_u_t)*(abs(dev_u_t)^b)) %>%
    mutate(pred = mu + b_i_reg + b_u_reg +dev_u_t) %>%
    pull(pred)
  return(RMSE(predicted_ratings, test_set$rating))
}


## dev2 function to calculate β(u) on dev_u(t) (temporal effect on user bias) according to the BellKor Solution pdf
dev2<-function(b){
  movie_avgs_reg<- train_set %>% 
    group_by(movieId) %>% 
    summarize(b_i_reg = sum(rating - mu)/(n()+l1),first_rate_i=min(date))
  
  date_user_avgs_reg <- train_set %>% 
    left_join(movie_avgs_reg, by='movieId') %>%
    group_by(userId) %>%
    summarize(b_u_reg = sum(rating - mu - b_i_reg)/(n()+l2),t_u = mean(date))
  
  predicted_ratings <- test_set%>% mutate(date = round_date(date, unit = "day")) %>%
    left_join(movie_avgs_reg, by='movieId') %>%
    left_join(date_user_avgs_reg, by='userId') %>%
    mutate(dev_u_t=as.numeric(difftime(date,t_u,units="days"))) %>%
    mutate(dev_u_t=a*sign(dev_u_t)*(abs(dev_u_t)^b)) %>%
    mutate(pred = mu + b_i_reg + b_u_reg +dev_u_t) %>%
    pull(pred)
  return(RMSE(predicted_ratings, test_set$rating))
}


```

\newpage

 Harvardx Team Code provided to build original dataset is the following one: 

```{r loading-code, message=FALSE}

##########################################################
# Create edx set, validation set (final hold-out test set)
##########################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)



```


\newpage  

## Exploratory Analysis


The edx dataset is a data.frame consisting of `r format(nrow(edx),big.mark=",",scientific=F)` rows and `r format(ncol(edx),big.mark=",",scientific=F)` columns, with ratings provided by a total of `r format(n_distinct(edx$userId), big.mark=",",scientific=F)` unique users for a total of `r format(n_distinct(edx$movieId),big.mark=",",scientific=F)` unique movies. Generally speaking cartesian product of these two sets would consist on about `r round((n_distinct(edx$userId)*n_distinct(edx$movieId))/1e+06)` million ratings. Hence, is clearly proved that not every user has rated every movie.


Taking a look at the distribution of the ratings

```{r Exploratory Analysis, message=FALSE}

edx %>%
  group_by(rating) %>%
  summarize(count = n()) %>%
  ggplot(aes(x = rating, y = count)) +
  geom_line() +
  ggtitle("Number of occurence of each rating")

```
we can appreciate several things:

    1. The rating given more often is 4*
    2. There is no 0* rating in this dataset
    3. There seems to be a patter to prefer integer ratings over half ratings