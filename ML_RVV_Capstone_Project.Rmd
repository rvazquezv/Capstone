---
title: '**ML algorithm for Harvardx Capstone Course**'
subtitle: 'HarvardX Data Science Professional Certificate: PH125.9x Capstone 1'
author: "Rubén Vázquez del Valle"
date: "4/8/2021"
output: html_document
---


## Introduction

For this project, I will create a movie recommendation system using 10M version of the MovieLens dataset to make the computation a little easier.

To build my origin datasets I have downloaded the MovieLens data previously described and run the code provided by Harvardx Team.

Once dataset were created I have followed Bell and Koren's paper describing their final solution to the Netflix Prize included in the next link: http://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf.



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Following libraries will be needed for the project:

```{r loading-libs, message=FALSE}

###############################################################################
##        Open required package libraries
###############################################################################

library(tidyverse)
library(ggplot2)
library(lubridate)
library(purrr)
library(stringr)
library(caret)
library(knitr)
library(scales)
library(RColorBrewer)
```

Following Functions will also be needed for the project

```{r loading-functions, message=FALSE}


###############################################################################
##       Functions specifically created for the project
###############################################################################


## RMSE function to compute Root Mean Squared errors between any pair of vectors 
RMSE <- function(true, predicted){
  sqrt(mean((true - predicted)^2))
}


## MAE function to compute Mean Absolute errors between any pair of vectors 
MAE <- function(true, predicted){
  mean(abs(true - predicted))
}


## substrRight function to substract n last characters on a string
substrRight <- function(x, n){
  substr(x, nchar(x)-n+1, nchar(x))
}


###############################################################################
##  Initialize parameters
###############################################################################
l1<-2.75
l2<-5
a<- -0.00075
b<- 0.4


## lambda1 function to regularize parameter λ1 according to The BellKor Solution pdf
lambda1<-function(l1){
  mu <- mean(train_set$rating)
  movie_avgs_reg <- train_set %>% 
    group_by(movieId) %>% 
    summarize(b_i = sum(rating - mu)/(n()+l1))
  
  predicted_ratings <- 
    test_set %>% 
    left_join(movie_avgs_reg, by='movieId') %>%
    mutate(pred = mu + b_i) %>%
    pull(pred)
  return(RMSE(predicted_ratings, test_set$rating))
}


## lambda2 function to regularize parameter λ2 according to The BellKor Solution pdf
lambda2<-function(l2){
  mu <- mean(train_set$rating)
  user_avgs <- train_set %>% 
    left_join(movie_avgs_reg, by='movieId') %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - mu - b_i_reg)/(n()+l2))
  
  predicted_ratings <- test_set %>% 
    left_join(movie_avgs_reg, by='movieId') %>%
    left_join(user_avgs, by='userId') %>%
    mutate(pred = mu + b_i_reg + b_u) %>%
    pull(pred)
  return(RMSE(predicted_ratings, test_set$rating))
}


## dev1 function to calculate α(u) on dev_u(t) (temporal effect on user bias) according to the BellKor Solution pdf 
dev1<-function(a){
  movie_avgs_reg<- train_set %>% 
    group_by(movieId) %>% 
    summarize(b_i_reg = sum(rating - mu)/(n()+l1),first_rate_i=min(date))
  
  date_user_avgs_reg <- train_set %>% 
    left_join(movie_avgs_reg, by='movieId') %>%
    group_by(userId) %>%
    summarize(b_u_reg = sum(rating - mu - b_i_reg)/(n()+l2),t_u = mean(date))
  
  predicted_ratings <- test_set%>% mutate(date = round_date(date, unit = "day")) %>%
    left_join(movie_avgs_reg, by='movieId') %>%
    left_join(date_user_avgs_reg, by='userId') %>%
    mutate(dev_u_t=as.numeric(difftime(date,t_u,units="days"))) %>%
    mutate(dev_u_t=a*sign(dev_u_t)*(abs(dev_u_t)^b)) %>%
    mutate(pred = mu + b_i_reg + b_u_reg +dev_u_t) %>%
    pull(pred)
  return(RMSE(predicted_ratings, test_set$rating))
}


## dev2 function to calculate β(u) on dev_u(t) (temporal effect on user bias) according to the BellKor Solution pdf
dev2<-function(b){
  movie_avgs_reg<- train_set %>% 
    group_by(movieId) %>% 
    summarize(b_i_reg = sum(rating - mu)/(n()+l1),first_rate_i=min(date))
  
  date_user_avgs_reg <- train_set %>% 
    left_join(movie_avgs_reg, by='movieId') %>%
    group_by(userId) %>%
    summarize(b_u_reg = sum(rating - mu - b_i_reg)/(n()+l2),t_u = mean(date))
  
  predicted_ratings <- test_set%>% mutate(date = round_date(date, unit = "day")) %>%
    left_join(movie_avgs_reg, by='movieId') %>%
    left_join(date_user_avgs_reg, by='userId') %>%
    mutate(dev_u_t=as.numeric(difftime(date,t_u,units="days"))) %>%
    mutate(dev_u_t=a*sign(dev_u_t)*(abs(dev_u_t)^b)) %>%
    mutate(pred = mu + b_i_reg + b_u_reg +dev_u_t) %>%
    pull(pred)
  return(RMSE(predicted_ratings, test_set$rating))
}


```

\newpage

 Harvardx Team Code provided to build original dataset is the following one: 

```{r loading-code, message=FALSE}

##########################################################
# Create edx set, validation set (final hold-out test set)
##########################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)



```


\newpage  

## Exploratory Analysis


Let's start by exploring and summarizing the edx dataset:


```{r Exploratory Analysis, message=FALSE}

head(edx)
summary(edx)

```


We can observe that edx dataset is a data.frame consisting of `r format(nrow(edx),big.mark=",",scientific=F)` rows and `r format(ncol(edx),big.mark=",",scientific=F)` columns, with ratings provided by a total of `r format(n_distinct(edx$userId), big.mark=",",scientific=F)` unique users (that I will refer as "u"s) for a total of `r format(n_distinct(edx$movieId),big.mark=",",scientific=F)` unique movies (that I will refer as "i"s). Generally speaking cartesian product of these two sets would consist on about `r round((n_distinct(edx$userId)*n_distinct(edx$movieId))/1e+06)` million ratings. Hence, it is definetively proved that not every user has rated every movie.

Clearly we are provided with a  little bit of pieces of information more than the movie, the user, and the ratings. There are three columns more adding the timestamp (date of the rating done by user u on movie i), the title (a character vector including the release year at the end of the string) and finally a segmentation of the movie by genre (actually, each movie can be included in more than one cathegory)

Taking a look at previous information and at the distribution of the ratings

```{r Exploratory Analysis 2, message=FALSE}

edx %>%
  group_by(rating) %>%
  summarize(count = n()) %>%
  ggplot(aes(x = rating, y = count)) +
  geom_line() +
  ggtitle("Number of occurence of each rating")

```


\n we can appreciate several things:

    1. The rating given more often is 4*
    2. There is no 0* rating in this dataset
    3. There seems to be a pattern to prefer integer ratings over rational ratings


Taking a look at the distribution of the number of ratings per movie

```{r Exploratory Analysis 3, message=FALSE}

edx %>%
  group_by(movieId) %>%
  summarize(n = n())%>%
  filter(n<5000) %>%
  ggplot(aes(x = n)) +
  geom_histogram(bins=15)  + scale_fill_brewer(palette="Blues") +
  ggtitle("Distribution of number of ratings per movie")


```

and at the distribution of the number of ratings per user


```{r Exploratory Analysis 4, message=FALSE}

edx %>%
  group_by(userId) %>%
  summarize(n = n())%>%
  ggplot(aes(x = n)) +
  geom_histogram(bins=15)  + scale_fill_brewer(palette="Blues") +
  ggtitle("Distribution of number of ratings per movie")


```

\n it is obvious that:



    5. Some movies get rated more than others, and
    6. Some users are more active than others
    

Hence, from this introductory analysis, some transformations seem to be needed on edx dataset


\newpage  

## Transforming and partitioning edx dataset

Timestamp is not in a human readable format and release year is included in title column, so let's transform timestamp into a more suitable format and extract release year from title to a new column called year

```{r Transforming edx dataset, message=FALSE}


edx<-edx %>% mutate( date = as_datetime(timestamp),year=as.numeric(str_extract(str_extract(substrRight(title,6),"\\([^()]+.\\d"),"\\d+\\d")))
head(edx)

```


So now I am ready to create a train and a test set to assess the accuracy of the models I will implement. 

```{r Partitioning edx dataset, message=FALSE}

## Selecting a random seed to allow replicability
set.seed(1978, sample.kind="Rounding")


## Creating training and testing partitions on edx movielens dataset 

test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.2, 
                                  list = FALSE)
train_set <- edx[-test_index,]
test_set <- edx[test_index,]


## Making testing partition comparable by taking out movies and users not present on training partition   

test_set <-test_set %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")


```

And finally we can start to implement our candidate models

\newpage  

## Modelling

Bell and Koren's paper describing their final solution reflects on different machine learning techniques:

    I.   BASELINE PREDICTORS
    II.  MATRIX FACTORIZATION WITH TEMPORAL DYNAMICS
    III. NEIGHBORHOOD MODELS WITH TEMPORAL DYNAMICS
    IV.  EXTENSIONS TO RESTRICTED BOLTZMANN MACHINES
    V.   GRADIENT BOOSTING DECISSION TRESS BLENDING
    
concluding


    The science of recommender systems is a prime beneficiary of the contest. Many new people became involved in the field and made their contributions. There is a clear spike in related publications, and the Netflix dataset is the direct catalyst to developing some of the better algorithms known in the field. Out of the numerous new algorithmic contributions, I would like to highlight one – those humble baseline predictors (or biases), which capture main effects in the data. While the literature mostly concentrates on the more sophisticated algorithmic aspects, we have learned that an accurate treatment of main effects is probably at least as significant as coming up with modeling breakthroughs.
