---
title: '**ML algorithm for Harvardx Capstone Course**'
subtitle: 'HarvardX Data Science Professional Certificate: PH125.9x Capstone 1'
author: "Rubén Vázquez del Valle"
date: "4/8/2021"
output:
  html_document:
    df_print: kable
    fig_caption: true
    toc: true
    latex_engine: xelatex
---


# 1.Introduction

For this project, I will create a movie recommendation system using 10M version of the MovieLens dataset to make the computation a little easier.

To build my origin datasets I have downloaded the MovieLens data previously described and run the code provided by Harvardx Team.

Once dataset was created I have followed Bell and Koren's paper describing their final solution to the Netflix Prize included in the next link: http://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf.



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE)
```


Following libraries will be needed for the project:

```{r loading-libs, message=FALSE}

###############################################################################
##        Open required package libraries
###############################################################################

library(tidyverse)
library(ggplot2)
library(lubridate)
library(purrr)
library(stringr)
library(caret)
library(knitr)
library(scales)
library(RColorBrewer)
library(dplyr)
```

Following Functions will also be needed for the project

```{r loading-functions, message=FALSE}


###############################################################################
##       Functions specifically created for the project
###############################################################################


## RMSE function to compute Root Mean Squared errors between any pair of vectors 
RMSE <- function(true, predicted){
  sqrt(mean((true - predicted)^2))
}


## MAE function to compute Mean Absolute errors between any pair of vectors 
MAE <- function(true, predicted){
  mean(abs(true - predicted))
}


## substrRight function to substract n last characters on a string
substrRight <- function(x, n){
  substr(x, nchar(x)-n+1, nchar(x))
}


## rating_stats_bygenre function to calculate statistics on a data frame df by genre s
rating_stats_bygenre<-function(df,avg,s){
  aux<- df  %>% filter(grepl(s,genres)) %>%  summarize(n=n(),mu=mean(rating),sigma=sd(rating),b_g=mean(rating - avg - b_i_reg - b_u_reg - ifelse(is.na(b_i),0,b_i) - dev_u_t))
  tibble(genre=s,n=aux$n,mu=aux$mu,sigma=aux$sigma,b_g=aux$b_g)  
}

## agg_sum_b_g to aggregate genre bias on genre g for those movies belonging to more than one genre
agg_sum_b_g<-function(df,g){
  auxdf<- df  %>% filter(grepl(g,genres)) %>% mutate(new_b_g=all_genres_stats$b_g[which(all_genres_stats$genre==g)]) %>% select(userId,movieId,new_b_g)
  df<- left_join(df, auxdf, by = c("userId","movieId")) %>% mutate(sum_b_g=ifelse(is.na(new_b_g),sum_b_g,sum_b_g+new_b_g)) %>% select(-new_b_g)
}


###############################################################################
##  Initialize parameters
###############################################################################
l1<-2.75
l2<-5
a<- -0.00015
b<- 0.4
n1<-780
###############################################################################


## lambda1 function to regularize parameter λ1 according to The BellKor Solution pdf
lambda1<-function(l1){
  mu <- mean(train_set$rating)
  movie_avgs_reg <- train_set %>% 
    group_by(movieId) %>% 
    summarize(b_i = sum(rating - mu)/(n()+l1))
  
  predicted_ratings <- 
    test_set %>% 
    left_join(movie_avgs_reg, by='movieId') %>%
    mutate(pred = mu + b_i) %>%
    pull(pred)
  return(RMSE(predicted_ratings, test_set$rating))
}


## lambda2 function to regularize parameter λ2 according to The BellKor Solution pdf
lambda2<-function(l2){
  mu <- mean(train_set$rating)
  user_avgs <- train_set %>% 
    left_join(movie_avgs_reg, by='movieId') %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - mu - b_i_reg)/(n()+l2))
  
  predicted_ratings <- test_set %>% 
    left_join(movie_avgs_reg, by='movieId') %>%
    left_join(user_avgs, by='userId') %>%
    mutate(pred = mu + b_i_reg + b_u) %>%
    pull(pred)
  return(RMSE(predicted_ratings, test_set$rating))
}

## dev1 function to calculate α(u) on dev_u(t) (temporal effect on user bias) according to the BellKor Solution pdf after applying  bi(t) = bi +bi,Bin(t)
dev1<-function(a){
  movie_avgs_reg<- train_set %>% 
    group_by(movieId) %>% 
    summarize(b_i_reg = sum(rating - mu)/(n()+l1),first_rate_i=min(date))
  
  date_user_avgs_reg <- train_set %>% 
    left_join(movie_avgs_reg, by='movieId') %>%
    group_by(userId) %>%
    summarize(b_u_reg = sum(rating - mu - b_i_reg)/(n()+l2),t_u = mean(date))
  
  movie_avgs_reg_bin<-train_set %>% 
    left_join(movie_avgs_reg, by='movieId') %>%
    left_join(date_user_avgs_reg, by='userId')  %>%
    mutate(bin=case_when(difftime(date,first_rate_i,units="days")<=n ~ 1,
                         difftime(date,first_rate_i,units="days")>n & difftime(date,first_rate_i,units="days")<=2*n ~ 2,
                         difftime(date,first_rate_i,units="days")>2*n & difftime(date,first_rate_i,units="days")<=3*n ~ 3,
                         difftime(date,first_rate_i,units="days")>3*n & difftime(date,first_rate_i,units="days")<=4*n ~ 4,
                         difftime(date,first_rate_i,units="days")>4*n & difftime(date,first_rate_i,units="days")<=5*n ~ 5,
                         difftime(date,first_rate_i,units="days")>5*n & difftime(date,first_rate_i,units="days")<=6*n ~ 6,
                         difftime(date,first_rate_i,units="days")>6*n & difftime(date,first_rate_i,units="days")<=7*n ~ 7,
                         difftime(date,first_rate_i,units="days")>7*n & difftime(date,first_rate_i,units="days")<=8*n ~ 8,
                         difftime(date,first_rate_i,units="days")>8*n & difftime(date,first_rate_i,units="days")<=9*n ~ 9,
                         difftime(date,first_rate_i,units="days")>9*n~10)
    )  %>%  group_by(movieId,bin) %>% 
    summarize(b_i = mean(rating - mu - b_i_reg - b_u_reg))
  
  
  predicted_ratings <- test_set %>% 
    left_join(movie_avgs_reg, by='movieId') %>%
    mutate(bin=case_when(difftime(date,first_rate_i,units="days")<=n ~ 1,
                         difftime(date,first_rate_i,units="days")>n & difftime(date,first_rate_i,units="days")<=2*n ~ 2,
                         difftime(date,first_rate_i,units="days")>2*n & difftime(date,first_rate_i,units="days")<=3*n ~ 3,
                         difftime(date,first_rate_i,units="days")>3*n & difftime(date,first_rate_i,units="days")<=4*n ~ 4,
                         difftime(date,first_rate_i,units="days")>4*n & difftime(date,first_rate_i,units="days")<=5*n ~ 5,
                         difftime(date,first_rate_i,units="days")>5*n & difftime(date,first_rate_i,units="days")<=6*n ~ 6,
                         difftime(date,first_rate_i,units="days")>6*n & difftime(date,first_rate_i,units="days")<=7*n ~ 7,
                         difftime(date,first_rate_i,units="days")>7*n & difftime(date,first_rate_i,units="days")<=8*n ~ 8,
                         difftime(date,first_rate_i,units="days")>8*n & difftime(date,first_rate_i,units="days")<=9*n ~ 9,
                         difftime(date,first_rate_i,units="days")>9*n~10)
    )  %>%
    left_join(date_user_avgs_reg, by='userId')%>%
    left_join(movie_avgs_reg_bin, by=c('movieId','bin'))%>%
    mutate(dev_u_t=as.numeric(difftime(date,t_u,units="days"))) %>%
    mutate(dev_u_t=a*sign(dev_u_t)*(abs(dev_u_t)^b)) %>%
    mutate(pred = mu + b_i_reg + b_u_reg + ifelse(is.na(b_i),0,b_i)+dev_u_t) %>%         
    pull(pred)
  return(RMSE(predicted_ratings, test_set$rating))
}


## dev2 function to calculate β(u) on dev_u(t) (temporal effect on user bias) according to the BellKor Solution pdf after applying movies bias reg as time function bi(t) = bi +bi,Bin(t)
dev2<-function(b){
  movie_avgs_reg<- train_set %>% 
    group_by(movieId) %>% 
    summarize(b_i_reg = sum(rating - mu)/(n()+l1),first_rate_i=min(date))
  
  date_user_avgs_reg <- train_set %>% 
    left_join(movie_avgs_reg, by='movieId') %>%
    group_by(userId) %>%
    summarize(b_u_reg = sum(rating - mu - b_i_reg)/(n()+l2),t_u = mean(date))
  
  movie_avgs_reg_bin<-train_set %>% 
    left_join(movie_avgs_reg, by='movieId') %>%
    left_join(user_avgs_reg, by='userId')  %>%
    mutate(bin=case_when(difftime(date,first_rate_i,units="days")<=n ~ 1,
                         difftime(date,first_rate_i,units="days")>n & difftime(date,first_rate_i,units="days")<=2*n ~ 2,
                         difftime(date,first_rate_i,units="days")>2*n & difftime(date,first_rate_i,units="days")<=3*n ~ 3,
                         difftime(date,first_rate_i,units="days")>3*n & difftime(date,first_rate_i,units="days")<=4*n ~ 4,
                         difftime(date,first_rate_i,units="days")>4*n & difftime(date,first_rate_i,units="days")<=5*n ~ 5,
                         difftime(date,first_rate_i,units="days")>5*n & difftime(date,first_rate_i,units="days")<=6*n ~ 6,
                         difftime(date,first_rate_i,units="days")>6*n & difftime(date,first_rate_i,units="days")<=7*n ~ 7,
                         difftime(date,first_rate_i,units="days")>7*n & difftime(date,first_rate_i,units="days")<=8*n ~ 8,
                         difftime(date,first_rate_i,units="days")>8*n & difftime(date,first_rate_i,units="days")<=9*n ~ 9,
                         difftime(date,first_rate_i,units="days")>9*n~10)
    )  %>% group_by(movieId,bin) %>% 
    summarize(b_i = mean(rating - mu - b_i_reg - b_u_reg))
  
  
  predicted_ratings <- test_set %>% 
    left_join(movie_avgs_reg, by='movieId') %>%
    mutate(bin=case_when(difftime(date,first_rate_i,units="days")<=n ~ 1,
                         difftime(date,first_rate_i,units="days")>n & difftime(date,first_rate_i,units="days")<=2*n ~ 2,
                         difftime(date,first_rate_i,units="days")>2*n & difftime(date,first_rate_i,units="days")<=3*n ~ 3,
                         difftime(date,first_rate_i,units="days")>3*n & difftime(date,first_rate_i,units="days")<=4*n ~ 4,
                         difftime(date,first_rate_i,units="days")>4*n & difftime(date,first_rate_i,units="days")<=5*n ~ 5,
                         difftime(date,first_rate_i,units="days")>5*n & difftime(date,first_rate_i,units="days")<=6*n ~ 6,
                         difftime(date,first_rate_i,units="days")>6*n & difftime(date,first_rate_i,units="days")<=7*n ~ 7,
                         difftime(date,first_rate_i,units="days")>7*n & difftime(date,first_rate_i,units="days")<=8*n ~ 8,
                         difftime(date,first_rate_i,units="days")>8*n & difftime(date,first_rate_i,units="days")<=9*n ~ 9,
                         difftime(date,first_rate_i,units="days")>9*n~10)
    )  %>%
    left_join(date_user_avgs_reg, by='userId')%>%
    left_join(movie_avgs_reg_bin, by=c('movieId','bin'))%>%
    mutate(dev_u_t=as.numeric(difftime(date,t_u,units="days"))) %>%
    mutate(dev_u_t=a*sign(dev_u_t)*(abs(dev_u_t)^b)) %>%
    mutate(pred = mu + b_i_reg + b_u_reg + ifelse(is.na(b_i),0,b_i)+dev_u_t) %>%         
    pull(pred)
  return(RMSE(predicted_ratings, test_set$rating))
}


## bins function to calculate optimal values on bins for movies bias regularized as a function of time segmented in bins
bins<-function(n){
movie_avgs_reg<- train_set %>% 
    group_by(movieId) %>% 
    summarize(b_i_reg = sum(rating - mu)/(n()+l1),first_rate_i=min(date))
  
date_user_avgs_reg <- train_set %>% 
    left_join(movie_avgs_reg, by='movieId') %>%
    group_by(userId) %>%
    summarize(b_u_reg = sum(rating - mu - b_i_reg)/(n()+l2),t_u = mean(date))

movie_avgs_reg_bin<-train_set %>% 
  left_join(movie_avgs_reg, by='movieId') %>%
  left_join(user_avgs_reg, by='userId')  %>%
  mutate(bin=case_when(difftime(date,first_rate_i,units="days")<=n ~ 1,
                       difftime(date,first_rate_i,units="days")>n & difftime(date,first_rate_i,units="days")<=2*n ~ 2,
                       difftime(date,first_rate_i,units="days")>2*n & difftime(date,first_rate_i,units="days")<=3*n ~ 3,
                       difftime(date,first_rate_i,units="days")>3*n & difftime(date,first_rate_i,units="days")<=4*n ~ 4,
                       difftime(date,first_rate_i,units="days")>4*n & difftime(date,first_rate_i,units="days")<=5*n ~ 5,
                       difftime(date,first_rate_i,units="days")>5*n & difftime(date,first_rate_i,units="days")<=6*n ~ 6,
                       difftime(date,first_rate_i,units="days")>6*n & difftime(date,first_rate_i,units="days")<=7*n ~ 7,
                       difftime(date,first_rate_i,units="days")>7*n & difftime(date,first_rate_i,units="days")<=8*n ~ 8,
                       difftime(date,first_rate_i,units="days")>8*n & difftime(date,first_rate_i,units="days")<=9*n ~ 9,
                       difftime(date,first_rate_i,units="days")>9*n~10)
  )%>%
  group_by(movieId,bin) %>% 
  summarize(b_i = mean(rating - mu - b_i_reg - b_u_reg))

predicted_ratings <- test_set %>% 
  left_join(movie_avgs_reg, by='movieId') %>%
  mutate(bin=case_when(difftime(date,first_rate_i,units="days")<=n ~ 1,
                       difftime(date,first_rate_i,units="days")>n & difftime(date,first_rate_i,units="days")<=2*n ~ 2,
                       difftime(date,first_rate_i,units="days")>2*n & difftime(date,first_rate_i,units="days")<=3*n ~ 3,
                       difftime(date,first_rate_i,units="days")>3*n & difftime(date,first_rate_i,units="days")<=4*n ~ 4,
                       difftime(date,first_rate_i,units="days")>4*n & difftime(date,first_rate_i,units="days")<=5*n ~ 5,
                       difftime(date,first_rate_i,units="days")>5*n & difftime(date,first_rate_i,units="days")<=6*n ~ 6,
                       difftime(date,first_rate_i,units="days")>6*n & difftime(date,first_rate_i,units="days")<=7*n ~ 7,
                       difftime(date,first_rate_i,units="days")>7*n & difftime(date,first_rate_i,units="days")<=8*n ~ 8,
                       difftime(date,first_rate_i,units="days")>8*n & difftime(date,first_rate_i,units="days")<=9*n ~ 9,
                       difftime(date,first_rate_i,units="days")>9*n~10)
  )  %>%
  left_join(date_user_avgs_reg, by='userId')%>%
  left_join(movie_avgs_reg_bin, by=c('movieId','bin'))%>%
  mutate(pred = mu + b_i_reg + b_u_reg + ifelse(is.na(b_i),0,b_i)) %>%         
  pull(pred)
return(RMSE(predicted_ratings, test_set$rating))
}

```


\newpage

 Harvardx Team Code provided to build original dataset is the following one: 

```{r loading-code, message=FALSE}

##########################################################
# Create edx set, validation set (final hold-out test set)
##########################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)



```
\newpage  

# 2.Exploratory Analysis

Let's start by exploring and summarizing the edx dataset:


```{r Exploratory Analysis, message=FALSE}

head(edx)
summary(edx)

```


It can be observed that edx dataset is a data.frame containing `r format(nrow(edx),big.mark=",",scientific=F)` rows and `r format(ncol(edx),big.mark=",",scientific=F)` columns, with ratings provided by a total of `r format(n_distinct(edx$userId), big.mark=",",scientific=F)` different users (that I will refer as "u"s) for a total of `r format(n_distinct(edx$movieId),big.mark=",",scientific=F)` different movies (that I will refer as "i"s). Generally speaking cartesian product of these two sets would consist on about `r round((n_distinct(edx$userId)*n_distinct(edx$movieId))/1e+06)` million ratings. Hence, it is definetively proved that not every user has rated every movie.

Clearly, there are more pieces of information than the movie, the user, and the ratings. There are three columns more including the timestamp (date of the rating done by user u on movie i), the title (a character vector including the title and the release year of the movie at the end of the string) and finally a segmentation of the movie by genre (actually, each movie can be included in more than one cathegory)

Taking a look at previous information and at the distribution of the ratings

```{r Exploratory Analysis 2, message=FALSE}

edx %>%
  group_by(rating) %>%
  summarize(count = n()) %>%
  ggplot(aes(x = rating, y = count)) +
  geom_line() +
  ggtitle("Number of occurence of each rating") +
  labs(caption = "Figure 1")

```


 it can be appreciated several things:

    1. The rating given more often is 4*
    2. There is no 0* rating in this dataset
    3. There seems to be a pattern to prefer integer ratings over rational ratings


Taking a look at the distribution of the number of ratings per movie

```{r Exploratory Analysis 3, message=FALSE}

edx %>%
  group_by(movieId) %>%
  summarize(n = n())%>%
  ggplot(aes(x = n)) +
  geom_histogram(bins=15)  + scale_fill_brewer(palette="Blues") +  scale_x_log10()+
  ggtitle("Distribution of number of ratings per movie") +
  labs(caption = "Figure 2")


```

and at the distribution of the number of ratings per user


```{r Exploratory Analysis 4, message=FALSE}

edx %>%
  group_by(userId) %>%
  summarize(n = n())%>%
  ggplot(aes(x = n)) +
  geom_histogram(bins=15)  + scale_fill_brewer(palette="Blues") + scale_x_log10()+
  ggtitle("Distribution of number of ratings per user") +
  labs(caption = "Figure 3")


```

 it is obvious that:



    5. Some movies get rated more than others, and
    6. Some users are more active than others
    

Hence, from this introductory analysis, some transformations seem to be needed on edx dataset


\newpage  

# 3.Transforming and partitioning edx dataset

Timestamp is not in a human readable format and release year is included in title column, so let's transform timestamp into a more suitable format and store it into a column called date and extract release year from title to a new column called year

```{r Transforming edx dataset, message=FALSE}


edx<-edx %>% mutate( date = as_datetime(timestamp),year=as.numeric(str_extract(str_extract(substrRight(title,6),"\\([^()]+.\\d"),"\\d+\\d")))
head(edx)

```


Genre column includes more than one available genre for each movie, so when calculating the effects generated by genres it will be necessary to aggregate several effects, if a movie belongs to more than one genre. With such purpose I created a couple of specific functions that will be disscused later and added a new column sum_b_g, initialized to 0 that will contain, $Σb_{g}$, the sum of all genre effects that each movie can be included into and will be calculated when dealing with them specifically
```{r Transforming edx dataset 2, message=FALSE}

edx<-edx %>% mutate(sum_b_g=0)
```




Now, I am ready to create a train and a test set to assess the accuracy of the models I will implement. 

```{r Partitioning edx dataset, message=FALSE}

## Selecting a random seed to allow replicability
set.seed(1978, sample.kind="Rounding")


## Creating training and testing partitions on edx movielens dataset 

test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.2, 
                                  list = FALSE)
train_set <- edx[-test_index,]
test_set <- edx[test_index,]


## Making testing partition comparable by taking out movies and users not present on training partition   

test_set <-test_set %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")


```

And finally I can start to implement the candidate models

\newpage  

# 4.Modelling

Bell and Koren's paper describing their final solution reflects on different machine learning techniques:

    I   BASELINE PREDICTORS
    II  MATRIX FACTORIZATION WITH TEMPORAL DYNAMICS
    III NEIGHBORHOOD MODELS WITH TEMPORAL DYNAMICS
    IV  EXTENSIONS TO RESTRICTED BOLTZMANN MACHINES
    V   GRADIENT BOOSTING DECISSION TRESS BLENDING
    
concluding


    The science of recommender systems is a prime beneficiary of the contest. Many new people became involved in the field and made their contributions. There is a clear spike in related publications, and the Netflix dataset is the direct catalyst to developing some of the better algorithms known in the field. Out of the numerous new algorithmic contributions, I would like to highlight one – those humble baseline predictors (or biases), which capture main effects in the data. While the literature mostly concentrates on the more sophisticated algorithmic aspects, we have learned that an accurate treatment of main effects is probably at least as significant as coming up with modeling breakthroughs.
    
    
Through this project I will try to follow some of this techniques and the explanations given in *"HarvardX - PH125.8x course: Data Science - Machine Learning"* specially on recommendation systems chapter, to obtain a model as good as possible, according to my skills 


## 4.1 BASELINE PREDICTORS


In doing so, the first baseline stage will be the naive one, just only calculating the average stars

$$Y_{u,i}=\mu+\epsilon_{u,i}$$  

```{r Baseline 1 Naive, message=FALSE}
###############################################################################
##        Modeling
##        I.) BASELINE
###############################################################################

################## 1. Adding Naive model
mu_hat <- mean(train_set$rating)
naive_rmse <- RMSE(test_set$rating, mu_hat)
naive_mae <- MAE(test_set$rating, mu_hat)

## Save results to table
Results <- tibble(method = "Naive", RMSE = naive_rmse, MAE = naive_mae)
Results

```

As expected results are still far from being acceptable. It seems clear that some movies tend to be rated better than others so to capture this effect related to movies I added a term $b_{i}$  and next step will be including it in the model as follows:

$$Y_{u,i}=\mu+b_{i}+\epsilon_{u,i}$$  

Statistics textbooks refer to the  bs as effects. However, in the Netflix challenge papers, they refer to them as “bias,” thus I have followed the same notation. 

```{r Baseline 2 movies bias, message=FALSE}
################## 2. Adding movies bias

mu <- mean(train_set$rating) 
movie_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))

qplot(b_i, data = movie_avgs, bins = 10, color = I("black")) +
  labs(caption = "Figure 4")


predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  mutate(pred = mu + b_i) %>%
  pull(pred)
movbias_rmse<-RMSE(predicted_ratings, test_set$rating)
movbias_mae<-MAE(predicted_ratings, test_set$rating)

## Save results to table
Results<-rbind(Results,tibble(method = "Movie Bias", RMSE = movbias_rmse, MAE = movbias_mae))
Results

```

Results seem better now but still far from desired ones

Following recommendations made by The BellKor Solution pdf, averages are shrunk towards zero by using regularization. Regularization constrains the total variability of the effect sizes by penalizing large estimates that come from small sample sizes. The BellKor Solution uses the regularization parameters, $λ_1,λ_2$, to regularize movies and users effects. Those parameters, $λ_1,λ_2$,  are determined by validation on the test set.

In fact the proposed formula for regularizing $b_{i}$ is:
$$ b_{i}=\frac{Σ_{u∈R(i)}·(r_{u,i}-\mu)}{λ_{1} + |R(i)|} $$

and  the proposed formula for regularizing  $b_{u}$ is:
$$ b_{u}=\frac{Σ_{i∈R(u)}·(r_{u,i}-\mu-b_{i})}{λ_{2} + |R(u)|} $$


For this reason I created the functions lambda1 and lambda2 which will allow me to identify the best values of $λ_1$ and $λ_2$. 

Finally, note that to prevent any unexpected problem such as syntax issues, I decided to initialize the values of all parameter

```{r Baseline 2.1 movies bias + regularization, message=FALSE}
################## 2.1. Adding movies bias regularized 

l <- seq(0, 20, 0.25)
l1<-l[which.min(sapply(l,lambda1))]
l1

movie_avgs_reg<- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i_reg = sum(rating - mu)/(n()+l1),first_rate_i=min(date))

predicted_ratings <- 
  test_set %>% 
  left_join(movie_avgs_reg, by='movieId') %>%
  mutate(pred = mu + b_i_reg) %>%
  pull(pred)

movbiasreg_rmse<-RMSE(predicted_ratings, test_set$rating)
movbiasreg_mae<-MAE(predicted_ratings, test_set$rating)


## Save results to table
Results<-rbind(Results,tibble(method = "Movie Bias regularized", RMSE = movbiasreg_rmse, MAE = movbiasreg_mae))
Results
```

Regularization does not seem to improve too much, but at least improves so I decided to keep it as part of my model.The same as with movie effect (movies bias), there seems to be clear that some users tend to be more critic than others so this effect on users will be the user bias and next step will be to calculate it:

$$Y_{u,i}=\mu+b_{i}+b_{u}+\epsilon_{u,i}$$  


```{r Baseline 3 movies bias reg + user bias, message=FALSE}
################## 3.Adding user bias

user_avgs <- train_set %>% 
  left_join(movie_avgs_reg, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i_reg))

qplot(b_u, data = user_avgs, bins = 10, color = I("black")) +
  labs(caption = "Figure 5")

predicted_ratings <- test_set %>% 
  left_join(movie_avgs_reg, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i_reg + b_u) %>%
  pull(pred)
movuserbias_rmse<-RMSE(predicted_ratings, test_set$rating)
movuserbias_mae<-MAE(predicted_ratings, test_set$rating)

## Save results to table
Results<-rbind(Results,tibble(method = "Movie Bias reg + User bias", RMSE = movuserbias_rmse,MAE = movuserbias_mae))
Results

```


Results are still improving, but I will try to do it better. So let`s try also regularization on user bias 

```{r Baseline 3.1 movies bias reg + user bias reg, message=FALSE}
################## 3.1. Adding user bias regularized 

l <- seq(5, 25, 1)
l2<-l[which.min(sapply(l,lambda2))]
l2

user_avgs_reg <- train_set %>% 
  left_join(movie_avgs_reg, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u_reg = sum(rating - mu - b_i_reg)/(n()+l2),t_u = mean(date))

predicted_ratings <- test_set %>% 
  left_join(movie_avgs_reg, by='movieId') %>%
  left_join(user_avgs_reg, by='userId') %>%
  mutate(pred = mu + b_i_reg + b_u_reg) %>%
  pull(pred)
movuserbiasreg_rmse<-RMSE(predicted_ratings, test_set$rating)
movuserbiasreg_mae<-MAE(predicted_ratings, test_set$rating)

## Save results to table
Results<-rbind(Results,tibble(method = "Movie Bias reg + User bias reg", RMSE = movuserbiasreg_rmse,MAE = movuserbiasreg_mae))
Results

```

The same as movie bias regularization, user bias regularization does not seem to improve too much, but at least improves a little bit so I also decided to keep it as part of my model.

The paper made by the “BellKor’s Pragmatic Chaos” final solution states that the users tend to change their baseline ratings over time including a natural drift in a user’s rating scale, and besides, item’s popularity may also change over time. I agree on these assumptions as some of these time effects can be observed on the next plots, specially for the movies that came after 1993

```{r Baseline reflexion on time effect, message=FALSE}

edx %>% mutate(date = round_date(date, unit = "week")) %>%
  group_by(date) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(date, rating)) +
  geom_point() +
  geom_smooth() +
  labs(caption = "Figure 6")

edx %>% mutate(date = round_date(date, unit = "month")) %>%
  group_by(date) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(date, rating)) +
  geom_point() +
  geom_smooth() +
  labs(caption = "Figure 7")

edx %>% 
  filter(year >= 1993) %>%
  group_by(movieId) %>%
  summarize(n = n(), years = 2018 - first(year),
            title = title[1],
            rating = mean(rating)) %>%
  mutate(rate = n/years) %>%
  ggplot(aes(rate, rating)) +
  geom_point() +
  geom_smooth()+
  labs(caption = "Figure 8")

edx %>% group_by(movieId) %>%
  summarize(n = n(), year = as.character(first(year))) %>%
  qplot(year, n, data = ., geom = "boxplot") +
  coord_trans(y = "sqrt") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  labs(caption = "Figure 9")

```


so following their path I have built first of all time bias $b_{i}$ and secondly user bias $b_{u}$ as functions of time.

Firstly, regarding $b_{i}$, they decided to split the item biases into time-based bins, using a constant item bias for each time period:
$$b_{i}(t) = bi +b_{i,Bin(t)}$$

In their paper they state that to choose the bins they have considered periods of time of around thirty weeks, but these considerations do not work well enough in my case so I created bins function assuming an arbitrary number of 10 bins that optimize the time split between each ot them as follows

```{r Bin election optimization, message=FALSE}
n <- seq(380, 830, 50)
n1<-sapply(n,bins)
qplot(n,n1) + labs(caption = "Figure 10")
n1<-n[which.min(n1)]
```

so based on this approach I split bins in around `r format(n1,big.mark=",",scientific=F)` days each. To be completely right this  arbitrary number of bins should also be optimized, but due to the computational time taken and small profit obtained I decided to keep this arbitrary ones.



Secondly, regarding $b_{u}$, to calculate user biases as a function of time they used the next function:
$$dev_{u}(t)=α_{u}·sign(t −t_{u})·|t −t_{u}|^\beta$$
where α and β are determined by validation on the test set. In this case I took the same β referred in the paper and then tried to optimize α and β via dev1 and dev2 functions

Next pieces of code show these attempts


```{r Baseline  4.1  movie rating date bias, message=FALSE}
################## 4.1 Adding movies bias regularized as a function of time segmented in bins
## Following recommendations made by The BellKor Solution pdf bi(t) = bi +bi,Bin(t) 
movie_avgs_reg_bin<-train_set %>% 
  left_join(movie_avgs_reg, by='movieId') %>%
  left_join(user_avgs_reg, by='userId')  %>%
  mutate(bin=case_when(difftime(date,first_rate_i,units="days")<=n1 ~ 1,
                       difftime(date,first_rate_i,units="days")>n1 & difftime(date,first_rate_i,units="days")<=2*n1 ~ 2,
                       difftime(date,first_rate_i,units="days")>2*n1 & difftime(date,first_rate_i,units="days")<=3*n1 ~ 3,
                       difftime(date,first_rate_i,units="days")>3*n1 & difftime(date,first_rate_i,units="days")<=4*n1~ 4,
                       difftime(date,first_rate_i,units="days")>4*n1 & difftime(date,first_rate_i,units="days")<=5*n1 ~ 5,
                       difftime(date,first_rate_i,units="days")>5*n1 & difftime(date,first_rate_i,units="days")<=6*n1 ~ 6,
                       difftime(date,first_rate_i,units="days")>6*n1 & difftime(date,first_rate_i,units="days")<=7*n1 ~ 7,
                       difftime(date,first_rate_i,units="days")>7*n1 & difftime(date,first_rate_i,units="days")<=8*n1 ~ 8,
                       difftime(date,first_rate_i,units="days")>8*n1 & difftime(date,first_rate_i,units="days")<=9*n1 ~ 9,
                       difftime(date,first_rate_i,units="days")>9*n1~10)
  )  %>% group_by(movieId,bin) %>% 
  summarize(b_i = mean(rating - mu - b_i_reg - b_u_reg))

qplot(b_i, data = movie_avgs_reg_bin, bins = 10, color = I("black")) +
  labs(caption = "Figure 11")

predicted_ratings <- test_set %>% 
  left_join(movie_avgs_reg, by='movieId') %>%
  mutate(bin=case_when(difftime(date,first_rate_i,units="days")<=n1 ~ 1,
                       difftime(date,first_rate_i,units="days")>n1 & difftime(date,first_rate_i,units="days")<=2*n1 ~ 2,
                       difftime(date,first_rate_i,units="days")>2*n1 & difftime(date,first_rate_i,units="days")<=3*n1 ~ 3,
                       difftime(date,first_rate_i,units="days")>3*n1 & difftime(date,first_rate_i,units="days")<=4*n1 ~ 4,
                       difftime(date,first_rate_i,units="days")>4*n1 & difftime(date,first_rate_i,units="days")<=5*n1 ~ 5,
                       difftime(date,first_rate_i,units="days")>5*n1 & difftime(date,first_rate_i,units="days")<=6*n1 ~ 6,
                       difftime(date,first_rate_i,units="days")>6*n1 & difftime(date,first_rate_i,units="days")<=7*n1 ~ 7,
                       difftime(date,first_rate_i,units="days")>7*n1 & difftime(date,first_rate_i,units="days")<=8*n1 ~ 8,
                       difftime(date,first_rate_i,units="days")>8*n1 & difftime(date,first_rate_i,units="days")<=9*n1 ~ 9,
                       difftime(date,first_rate_i,units="days")>9*n1~10)
  )  %>%
  left_join(user_avgs_reg, by='userId')%>%
  left_join(movie_avgs_reg_bin, by=c('movieId','bin'))%>%
  mutate(pred = mu + b_i_reg + b_u_reg + ifelse(is.na(b_i),0,b_i)) %>%         
  pull(pred)
movbiasregbin_rmse<-RMSE(predicted_ratings, test_set$rating) 
movbiasregbin_mae<-MAE(predicted_ratings, test_set$rating)



## Save results to table
Results<-rbind(Results,tibble(method = "Movie Bias reg + User bias reg+ time effect on movie Bias reg", RMSE = movbiasregbin_rmse, MAE = movbiasregbin_mae))
Results
```

Indeed this time effect is allowing to improve a little bit also


```{r Baseline  4.2  user rating date bias , message=FALSE}
################## 4.2 Adding user rating date bias  

##
## Following recommendations made by The BellKor Solution pdf, the users tend to change their baseline ratings over time including
## a natural drift in a user’s rating scale, so we build user bias bu as a function of time dev_u(t) = α·sign(t −tu)·|t −tu|^β
## α,β are determined by validation on the test set

a<- seq(-0.0015,0.0005,0.00025)
b<-0.4                               # initial β proposed in The BellKor Solution paper
devut<-sapply(a,dev1)
a<-a[which.min(devut)]
a

b <- seq(0, 1,0.1)
devut<-sapply(b,dev2)
b<-b[which.min(devut)]
b


## We took advantage of user_avgs_reg not only for calculating b_u_reg but also t_u, so we do not need to calculate it here
predicted_ratings <- test_set %>% 
  left_join(movie_avgs_reg, by='movieId') %>%
  mutate(bin=case_when(difftime(date,first_rate_i,units="days")<=n1 ~ 1,
                       difftime(date,first_rate_i,units="days")>n1 & difftime(date,first_rate_i,units="days")<=2*n1 ~ 2,
                       difftime(date,first_rate_i,units="days")>2*n1 & difftime(date,first_rate_i,units="days")<=3*n1 ~ 3,
                       difftime(date,first_rate_i,units="days")>3*n1 & difftime(date,first_rate_i,units="days")<=4*n1 ~ 4,
                       difftime(date,first_rate_i,units="days")>4*n1 & difftime(date,first_rate_i,units="days")<=5*n1 ~ 5,
                       difftime(date,first_rate_i,units="days")>5*n1 & difftime(date,first_rate_i,units="days")<=6*n1 ~ 6,
                       difftime(date,first_rate_i,units="days")>6*n1 & difftime(date,first_rate_i,units="days")<=7*n1 ~ 7,
                       difftime(date,first_rate_i,units="days")>7*n1 & difftime(date,first_rate_i,units="days")<=8*n1 ~ 8,
                       difftime(date,first_rate_i,units="days")>8*n1 & difftime(date,first_rate_i,units="days")<=9*n1 ~ 9,
                       difftime(date,first_rate_i,units="days")>9*n1~10))  %>%
  left_join(user_avgs_reg, by='userId')%>%
  left_join(movie_avgs_reg_bin, by=c('movieId','bin'))%>%
  mutate(dev_u_t=as.numeric(difftime(date,t_u,units="days"))) %>%
  mutate(dev_u_t=a*sign(dev_u_t)*(abs(dev_u_t)^b)) %>%
  mutate(pred = mu + b_i_reg + b_u_reg + ifelse(is.na(b_i),0,b_i)+dev_u_t) %>%         
  pull(pred)
usertimebias_rmse<-RMSE(predicted_ratings, test_set$rating)
usertimebias_mae<-MAE(predicted_ratings, test_set$rating)

## Save results to table
Results<-rbind(Results,tibble(method = "Movie Bias reg + User bias reg+ time effect on movie Bias reg + dev_u(t)", RMSE = usertimebias_rmse,MAE = usertimebias_mae))
Results

```

At this point the “BellKor’s Pragmatic Chaos” final solution recalls that the number of ratings a user gave on a specific day explains a significant portion of the variability of the data during that day. Hence they introduce a “frequency” effect calculus, that counterintuitively, even though  solely driven by user u, it will influence the item-biases, rather than the user-biases. They also claim that such effect was quite dramatic in terms of reducing RMSE, but so far as today, all my attempts  have failed

Then, instead of capturing this effect, I followed  *"HarvardX - PH125.8x course: Data Science - Machine Learning"* suggestion on capturing the bias on genres as 
$$Y_{u,i}=\mu+b_{i}+b_{u}+Σx_{u,i,k}·\beta_{k}+\epsilon_{u,i}$$  

and mapping to my current model it turns into 

$$Y_{u,i}=\mu+b_{i}+b_{i,Bin(t)}+b_{u}+α_{u}·sign(t −t_{u})·|t −t_{u}|^\beta+Σx_{u,i,k}·\beta_{k}+\epsilon_{u,i}$$  


```{r Baseline  5.1   genre bias , message=FALSE}

genre_avgs<-  train_set %>% 
  left_join(movie_avgs_reg, by='movieId') %>%
  mutate(bin=case_when(difftime(date,first_rate_i,units="days")<=n1 ~ 1,
                       difftime(date,first_rate_i,units="days")>n1 & difftime(date,first_rate_i,units="days")<=2*n1 ~ 2,
                       difftime(date,first_rate_i,units="days")>2*n1 & difftime(date,first_rate_i,units="days")<=3*n1 ~ 3,
                       difftime(date,first_rate_i,units="days")>3*n1 & difftime(date,first_rate_i,units="days")<=4*n1 ~ 4,
                       difftime(date,first_rate_i,units="days")>4*n1 & difftime(date,first_rate_i,units="days")<=5*n1 ~ 5,
                       difftime(date,first_rate_i,units="days")>5*n1 & difftime(date,first_rate_i,units="days")<=6*n1 ~ 6,
                       difftime(date,first_rate_i,units="days")>6*n1 & difftime(date,first_rate_i,units="days")<=7*n1 ~ 7,
                       difftime(date,first_rate_i,units="days")>7*n1 & difftime(date,first_rate_i,units="days")<=8*n1 ~ 8,
                       difftime(date,first_rate_i,units="days")>8*n1 & difftime(date,first_rate_i,units="days")<=9*n1 ~ 9,
                       difftime(date,first_rate_i,units="days")>9*n1~10))  %>%
  left_join(user_avgs_reg, by='userId')%>%
  left_join(movie_avgs_reg_bin, by=c('movieId','bin'))%>%
  mutate(dev_u_t=as.numeric(difftime(date,t_u,units="days"))) %>%
  mutate(dev_u_t=a*sign(dev_u_t)*(abs(dev_u_t)^b))
```
Nevertheless, this time is not as straightforward as before since each movie can belong to several genres so I have taken off each genre for each movie, extracting all different existing genres, calculating afterwards average rating by genre and genre bias $b_{g}$. Aggregating different genre bias for each movie into a single bias $Σb_{g}$, and finally capturing this effect as follows

First of all let's extract all different existing genres

```{r Baseline  5.2   genre bias , message=FALSE}
all_genres<-genre_avgs  %>% select(genres) %>% group_by(genres) %>% summarize(n=n())  %>% pull(genres)
all_genres<-enframe(str_split(all_genres, pattern="\\|")) %>% unnest(value) %>% group_by(value) %>% summarize(n=n())
all_genres<-all_genres %>% mutate(genre=value) %>% select(genre)
all_genres
```

Secondly let's calculate average rating by genre and genre bias b_g through function rating_stats_bygenre

```{r Baseline  5.3   genre bias , message=FALSE}
all_genres_stats<-tibble(.rows = NULL)
for(i in (1:length(all_genres$genre))){
  all_genres_stats<-rbind.data.frame(all_genres_stats,rating_stats_bygenre(genre_avgs,mu,all_genres$genre[i]))
}

all_genres_stats %>% ggplot(aes(x=genre,y=mu)) +
  geom_point() +
  ylab("Average Ratings over general average") +
  geom_errorbar(aes(ymin=mu-sigma, ymax=mu+sigma,col=genre)) +
  geom_hline(yintercept=mu)+
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank())+
  labs(caption = "Figure 12")
```


And finally let's aggregate genre bias to test_set dataset for all genres present in each movie


```{r Baseline  5.4   genre bias , message=FALSE}
for(i in (1:length(all_genres$genre))){
  test_set<-agg_sum_b_g(test_set,all_genres$genre[i])
}

predicted_ratings <- test_set %>% 
  left_join(movie_avgs_reg, by='movieId') %>%
  mutate(bin=case_when(difftime(date,first_rate_i,units="days")<=n1 ~ 1,
                       difftime(date,first_rate_i,units="days")>n1 & difftime(date,first_rate_i,units="days")<=2*n1 ~ 2,
                       difftime(date,first_rate_i,units="days")>2*n1 & difftime(date,first_rate_i,units="days")<=3*n1 ~ 3,
                       difftime(date,first_rate_i,units="days")>3*n1 & difftime(date,first_rate_i,units="days")<=4*n1 ~ 4,
                       difftime(date,first_rate_i,units="days")>4*n1 & difftime(date,first_rate_i,units="days")<=5*n1 ~ 5,
                       difftime(date,first_rate_i,units="days")>5*n1 & difftime(date,first_rate_i,units="days")<=6*n1 ~ 6,
                       difftime(date,first_rate_i,units="days")>6*n1 & difftime(date,first_rate_i,units="days")<=7*n1 ~ 7,
                       difftime(date,first_rate_i,units="days")>7*n1 & difftime(date,first_rate_i,units="days")<=8*n1 ~ 8,
                       difftime(date,first_rate_i,units="days")>8*n1 & difftime(date,first_rate_i,units="days")<=9*n1 ~ 9,
                       difftime(date,first_rate_i,units="days")>9*n1~10))  %>%
  left_join(user_avgs_reg, by='userId')%>%
  left_join(movie_avgs_reg_bin, by=c('movieId','bin'))%>%
  mutate(dev_u_t=as.numeric(difftime(date,t_u,units="days"))) %>%
  mutate(dev_u_t=a*sign(dev_u_t)*(abs(dev_u_t)^b)) %>%
  mutate(pred = mu + b_i_reg + b_u_reg + ifelse(is.na(b_i),0,b_i)+dev_u_t+sum_b_g) %>%         
  pull(pred)
movusergenrebias_rmse<-RMSE(predicted_ratings, test_set$rating)
movusergenrebias_mae<-MAE(predicted_ratings, test_set$rating)

## Save results to table
Results<-rbind(Results,tibble(method = "Movie Bias reg + User bias reg+ time effect on movie Bias reg + dev_u(t) + Genre bias", RMSE = movusergenrebias_rmse,MAE = movusergenrebias_mae))
Results
```

Comparing Figure 4, Figure 5 and Figure 11 and also RMSE and MAE calculated for each of those process it is possible to figure out the influence of each of those bias on final predictions, and which effects seem to affect more to final predictions 


## 4.2 MATRIX FACTORIZATION


On his paper Yehuda Koren writes about improving baseline predictor by applying matrix factorization with temporal dynamics. He states they modeled SVD++ based on the following prediction rule:

$$ \hat{r}_{u,i} = b_{u,i} + q_{i}^T\Biggl(p_{u}(t_{u,i})+|N(u)|^{-1/2}\sum_{j€N(u)} y_{j} \Biggr)  $$

where $\hat{r}_{u,i}$ are the preditec ratings, $b_{u,i}$ the effect on users and movies previously explained on baseline model,  $|N(u)|$ the cardinality of the set of ratings provided by user u, and finally $q_{i},y_{i}$ and $p_{u}(t_{u,i})$ factors provided by matrix factorization that will capture the effects not explained yet

I will try to follow what they have done and check if it also improves my current baseline model. 

The typical way to perform matrix factorizations is to perform a singular value decomposition on the (sparse) ratings matrix. Rafael Irizarri on  *"HarvardX - PH125.8x course: Data Science - Machine Learning"* explained pretty well SVD and also introduced us to the way SVD was performed by different contestants on the Netflix challenge:


    The SVD and PCA are complicated concepts, but one way to understand them is that SVD is an algorithm that finds  the vectors p and q that permit us to rewrite the matrix r with m rows and n columns as:
    
$$ r_{u,i}= p_{u,1}·q_{1,i} + p_{u,2}·q_{2,i} + ... + p_{u,n}·q_{n,i}  $$

      with the variability of each term decreasing and with the ps uncorrelated. 
      The algorithm also computes this variability so that we can know how much of the matrices, total variability is explained as we add new terms. This may permit us to see that, with just a few terms, we can explain most of the variability. 


Hence I tried to follow this approach. First of all it is important to notice that there are  `r format(n_distinct(edx$userId), big.mark=",",scientific=F)` different users and  `r format(n_distinct(edx$movieId), big.mark=",",scientific=F)` different movies, so tranform the dataset into a matrix means building a   `r format(n_distinct(edx$userId), big.mark=",",scientific=F)`x`r format(n_distinct(edx$movieId), big.mark=",",scientific=F)` matrix that my laptop is not able to process. 



Thus I decided to create a smaller random sample of edx dataset,

```{r smaller sample-code, message=FALSE}
users<-distinct(edx,userId)%>%pull(userId)
movies<-distinct(edx,movieId)%>%pull(movieId)
set.seed(1978, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
sample_users<-as_tibble(sample(users,size=2*6987))%>%mutate(userId=value)%>%select(userId)
movie_users<-as_tibble(sample(movies,size=2*1068))%>%mutate(movieId=value)%>%select(movieId)
small_edx <-edx %>% 
  semi_join(movie_users, by = "movieId") %>%
  semi_join(sample_users, by = "userId")

summary(small_edx)
```




and apply my baseline model, but in this case for simplicity sake I have only applied movies and user effects,obtaining following results
 
 
 ```{r small_edx apply baseline model, message=FALSE,echo = FALSE}
## Preprocessing original dataset to add info
## Release Year is included in the title, adding a column with the year
small_edx<-small_edx %>% mutate( date = as_datetime(timestamp),year=as.numeric(str_extract(str_extract(substrRight(title,6),"\\([^()]+.\\d"),"\\d+\\d")))
## Selecting a random seed to allow replicability
set.seed(1978, sample.kind="Rounding")
## Creating training a testing partitions on edx movielens dataset 
test_index <- createDataPartition(y = small_edx$rating, times = 1, p = 0.2, 
                                  list = FALSE)
small_train_set <- small_edx[-test_index,]
small_test_set <- small_edx[test_index,]
## Making testing partition comparable by taking out movies and users not present on training partition   
small_test_set <-small_test_set %>% 
  semi_join(small_train_set, by = "movieId") %>%
  semi_join(small_train_set, by = "userId")
###############################################################################
## Small_Edx_baseline
###############################################################################
################## 1. Adding Naive model
mu_hat <- mean(small_train_set$rating)
naive_rmse <- RMSE(small_test_set$rating, mu_hat)
naive_mae <- MAE(small_test_set$rating, mu_hat)
## Save results to table
small_Results <- tibble(method = "Naive", RMSE = naive_rmse, MAE = naive_mae)
################## 2. Adding movies bias
small_mu <- mean(small_train_set$rating) 
small_movie_avgs <- small_train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - small_mu))
predicted_ratings <- small_test_set %>% 
  left_join(small_movie_avgs, by='movieId') %>%
  mutate(pred = small_mu + b_i) %>%
  pull(pred)
movbias_rmse<-RMSE(predicted_ratings, small_test_set$rating)
movbias_mae<-MAE(predicted_ratings, small_test_set$rating)
## Save results to table
small_Results<-rbind(small_Results,tibble(method = "Movie Bias", RMSE = movbias_rmse, MAE = movbias_mae))
################## 3.Adding user bias
small_user_avgs <- small_train_set %>% 
  left_join(small_movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - small_mu - b_i))
predicted_ratings <- small_test_set %>% 
  left_join(small_movie_avgs, by='movieId') %>%
  left_join(small_user_avgs, by='userId') %>%
  mutate(pred = small_mu + b_i + b_u) %>%
  pull(pred)
movuserbias_rmse<-RMSE(predicted_ratings, small_test_set$rating)
movuserbias_mae<-MAE(predicted_ratings, small_test_set$rating)
## Save results to table
small_Results<-rbind(small_Results,tibble(method = "Movie Bias  + User bias", RMSE = movuserbias_rmse,MAE = movuserbias_mae))
```

```{r smaller edx results, message=FALSE}
small_Results
```


comparing results with previous ones, despite not being equal, seem close enough to consider this approach as worhty  and finally prior to further efforts,  working on matrix factorization on this smaller matrix instead of the full matrix wich would be unmanageable with my current resources


Now instead of the original `r format(n_distinct(edx$userId), big.mark=",",scientific=F)`x`r format(n_distinct(edx$movieId), big.mark=",",scientific=F)` matrix, I will be working with a   `r format(n_distinct(small_edx$userId), big.mark=",",scientific=F)`x`r format(n_distinct(small_edx$movieId), big.mark=",",scientific=F)` matrix, that despite of being huge is manageable on my laptop and do not seem to lose to much information


At this point, I will obtain residuals from my baseline prediction model and transform residuals into a matrix I will apply SVD algorithm

```{r smaller matrix factorization 1 results, message=FALSE}

# Calculate residuals of my predictions 
predicted_ratings <- small_train_set %>% 
  left_join(small_movie_avgs, by='movieId') %>%
  left_join(small_user_avgs, by='userId') %>%
  mutate(pred = small_mu + b_i + b_u,resid=rating-pred)%>% 
  select(userId, movieId, resid)


# Once resids on my predictions have been calculated just transform them into a  matrix
y <-as_tibble(predicted_ratings)
z<- y %>% pivot_wider(names_from = "movieId", values_from = "resid") %>%
  as.matrix()

# Add rownames and columnanes to the matrix
rownames(z)<- z[,1]
z <- z[,-1]

movie_titles <- small_edx %>% 
  select(movieId, title) %>%
  distinct()

colnames(z) <- with(movie_titles, title[match(colnames(z), movieId)])


```

As expected due to the fact that not every user has rated every movie, there are thousand of empty values within the matrix. In order to be able to apply SVD I have replaced this missing values by zeros


```{r smaller matrix factorization 2 results, message=FALSE}
#Erase NA values and plot explanaition on variability of the sd
z[is.na(z)] <- 0

```


and afterwards applied Singular Value Decomposition algorithm

```{r smaller matrix factorization 3 results, message=FALSE}
#Applying Sigle Value Decomposition
s<-svd(z)

```

Thus, considering 50 eigenvalues we are able to explain `r format(100 * sum(s$d[1:50]^2) / sum(s$d^2), big.mark=",",scientific=F)` % of the variability. So I will re-build my prediction on residuals as follows: 

```{r smaller matrix factorization 4 results, message=FALSE}
z_hat <- with(s,sweep(u[, 1:50], 2, d[1:50], FUN="*") %*% t(v[, 1:50]))

#Transforming it from matrix to data.frame again
rownames(z_hat)<- rownames(z)
colnames(z_hat) <- colnames(z)
userId<-as.numeric(rownames(z_hat))
z_hat <- as_tibble(z_hat,rownames=NA) %>%   cbind(userId,.)  %>% 
  gather(data=.,-userId,key='title',value=resid) 


```

And finally using these predictions on residuals to forecast on testing dataset:


```{r smaller matrix factorization 5 results, message=FALSE}


predicted_ratings <- small_test_set %>% 
  left_join(small_movie_avgs, by='movieId') %>%
  left_join(small_user_avgs, by='userId') %>%
  left_join(z_hat, by=c('title','userId'))%>%
  mutate(pred = small_mu + b_i + b_u + ifelse(is.na(resid),0,resid)) %>%
  pull(pred)


resid_rmse<-RMSE(predicted_ratings, small_test_set$rating)
resid_mae<-MAE(predicted_ratings, small_test_set$rating)

small_Results<-rbind(small_Results,tibble(method = "Movie Bias  + User bias + Matrix Factorization", RMSE = resid_rmse,MAE = resid_mae))

small_Results
```

# 5.Validation




# 6.Conclusions



, but due to lack of computational resources, lack of data science better skills and time to improve I obtained the residuals of the baseline prediction, transformed it into a residual matrix $r_{u,i}$ and applied SVD
